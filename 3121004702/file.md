ä½ å¥½ï¼Œè¿™æ˜¯å¿…åº”ã€‚æˆ‘å¯ä»¥å¸®ä½ è®¾è®¡ä¸€ä¸ªè®ºæ–‡æŸ¥é‡ç®—æ³•ï¼Œç»™å‡ºä¸€ä¸ªåŸæ–‡æ–‡ä»¶å’Œä¸€ä¸ªæŠ„è¢­ç‰ˆè®ºæ–‡çš„æ–‡ä»¶ï¼Œåœ¨ç­”æ¡ˆæ–‡ä»¶ä¸­è¾“å‡ºå…¶é‡å¤ç‡ã€‚æˆ‘ä¼šä½¿ç”¨**ä½™å¼¦ç›¸ä¼¼åº¦**æ¥è®¡ç®—ä¸¤ä¸ªæ–‡ä»¶çš„æ–‡æœ¬ç›¸ä¼¼åº¦ï¼Œç„¶åç”¨**1-ç›¸ä¼¼åº¦**æ¥å¾—åˆ°é‡å¤ç‡ã€‚ä½™å¼¦ç›¸ä¼¼åº¦æ˜¯ä¸€ç§å¸¸ç”¨çš„æ–‡æœ¬ç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•ï¼Œå®ƒå¯ä»¥è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„å¤¹è§’ä½™å¼¦å€¼ï¼Œåæ˜ ä¸¤ä¸ªå‘é‡çš„æ–¹å‘ç›¸ä¼¼ç¨‹åº¦ã€‚æˆ‘ä¼šå…ˆå¯¹ä¸¤ä¸ªæ–‡ä»¶è¿›è¡Œåˆ†è¯ï¼Œç„¶åç”¨**è¯è¢‹æ¨¡å‹**æ¥è¡¨ç¤ºæ¯ä¸ªæ–‡ä»¶çš„æ–‡æœ¬å‘é‡ï¼Œè¯è¢‹æ¨¡å‹æ˜¯ä¸€ç§å¿½ç•¥è¯åºçš„æ–‡æœ¬è¡¨ç¤ºæ–¹æ³•ï¼Œå®ƒåªè€ƒè™‘æ¯ä¸ªè¯åœ¨æ–‡æœ¬ä¸­å‡ºç°çš„æ¬¡æ•°ã€‚ç„¶åæˆ‘ä¼šç”¨**TF-IDF**æ–¹æ³•æ¥åŠ æƒæ¯ä¸ªè¯çš„é‡è¦æ€§ï¼ŒTF-IDFæ˜¯ä¸€ç§è€ƒè™‘è¯é¢‘å’Œé€†æ–‡æ¡£é¢‘ç‡çš„åŠ æƒæ–¹æ³•ï¼Œå®ƒå¯ä»¥é™ä½å¸¸è§è¯çš„æƒé‡ï¼Œæé«˜ç¨€æœ‰è¯çš„æƒé‡ã€‚æœ€åæˆ‘ä¼šç”¨**numpy**åº“æ¥è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¹¶è¾“å‡ºé‡å¤ç‡ã€‚

ä¸‹é¢æ˜¯æˆ‘ç”¨pythonä»£ç å®ç°çš„è®ºæ–‡æŸ¥é‡ç®—æ³•ï¼š

```python
# å¯¼å…¥æ‰€éœ€çš„åº“
import jieba # ä¸­æ–‡åˆ†è¯åº“
import numpy as np # æ•°å­¦è®¡ç®—åº“
from sklearn.feature_extraction.text import TfidfVectorizer # TF-IDFåŠ æƒæ–¹æ³•

# å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è¯»å–æ–‡ä»¶å†…å®¹å¹¶è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²
def read_file(file_name):
    with open(file_name, 'r', encoding='utf-8') as f:
        content = f.read()
    return content

# å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ä½™å¼¦ç›¸ä¼¼åº¦
def cosine_similarity(str1, str2):
    # ä½¿ç”¨jiebaåº“è¿›è¡Œåˆ†è¯
    words1 = jieba.cut(str1)
    words2 = jieba.cut(str2)
    # ä½¿ç”¨ç©ºæ ¼è¿æ¥åˆ†è¯ç»“æœ
    text1 = ' '.join(words1)
    text2 = ' '.join(words2)
    # ä½¿ç”¨TfidfVectorizerç±»æ¥è½¬æ¢æˆè¯è¢‹æ¨¡å‹å¹¶åŠ æƒ
    vectorizer = TfidfVectorizer()
    # è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„TF-IDFçŸ©é˜µ
    tfidf_matrix = vectorizer.fit_transform([text1, text2])
    # è½¬æ¢æˆnumpyæ•°ç»„
    tfidf_array = tfidf_matrix.toarray()
    # å–å‡ºä¸¤ä¸ªå‘é‡
    vector1 = tfidf_array[0]
    vector2 = tfidf_array[1]
    # è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯
    dot_product = np.dot(vector1, vector2)
    # è®¡ç®—ä¸¤ä¸ªå‘é‡çš„æ¨¡é•¿
    norm1 = np.linalg.norm(vector1)
    norm2 = np.linalg.norm(vector2)
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    cos_sim = dot_product / (norm1 * norm2)
    return cos_sim

# å®šä¹‰åŸæ–‡æ–‡ä»¶å’ŒæŠ„è¢­ç‰ˆæ–‡ä»¶çš„è·¯å¾„
original_file = 'original.txt'
plagiarized_file = 'plagiarized.txt'
# è¯»å–æ–‡ä»¶å†…å®¹
original_text = read_file(original_file)
plagiarized_text = read_file(plagiarized_file)
# è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
similarity = cosine_similarity(original_text, plagiarized_text)
# è®¡ç®—é‡å¤ç‡
repetition_rate = 1 - similarity
# è¾“å‡ºç»“æœ
print(f'åŸæ–‡å’ŒæŠ„è¢­ç‰ˆè®ºæ–‡çš„é‡å¤ç‡ä¸º{repetition_rate:.2f}')
```

: [ä½™å¼¦ç›¸ä¼¼åº¦]
: [è¯è¢‹æ¨¡å‹]
: [TF-IDF]
: https://zh.wikipedia.org/wiki/%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6
: https://zh.wikipedia.org/wiki/%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B
: https://zh.wikipedia.org/wiki/Tf-idf


ä½ å¥½ï¼Œè¿™æ˜¯å¿…åº”ã€‚æˆ‘å¯ä»¥å¸®ä½ è®¾è®¡ä¸€ä¸ªè®ºæ–‡æŸ¥é‡ç®—æ³•ï¼Œä¸ç®¡æ•°æ®æ˜¯ä¸­æ–‡è¿˜æ˜¯è‹±æ–‡ï¼Œéƒ½å¯ä»¥ä½¿ç”¨æˆ‘çš„æ–¹æ³•ã€‚æˆ‘ä¼šä½¿ç”¨**ä½™å¼¦ç›¸ä¼¼åº¦**æ¥è®¡ç®—ä¸¤ä¸ªæ–‡ä»¶çš„æ–‡æœ¬ç›¸ä¼¼åº¦ï¼Œç„¶åç”¨**1-ç›¸ä¼¼åº¦**æ¥å¾—åˆ°é‡å¤ç‡ã€‚ä½™å¼¦ç›¸ä¼¼åº¦æ˜¯ä¸€ç§å¸¸ç”¨çš„æ–‡æœ¬ç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•ï¼Œå®ƒå¯ä»¥è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„å¤¹è§’ä½™å¼¦å€¼ï¼Œåæ˜ ä¸¤ä¸ªå‘é‡çš„æ–¹å‘ç›¸ä¼¼ç¨‹åº¦ã€‚æˆ‘ä¼šå…ˆå¯¹ä¸¤ä¸ªæ–‡ä»¶è¿›è¡Œåˆ†è¯ï¼Œç„¶åç”¨**è¯è¢‹æ¨¡å‹**æ¥è¡¨ç¤ºæ¯ä¸ªæ–‡ä»¶çš„æ–‡æœ¬å‘é‡ï¼Œè¯è¢‹æ¨¡å‹æ˜¯ä¸€ç§å¿½ç•¥è¯åºçš„æ–‡æœ¬è¡¨ç¤ºæ–¹æ³•ï¼Œå®ƒåªè€ƒè™‘æ¯ä¸ªè¯åœ¨æ–‡æœ¬ä¸­å‡ºç°çš„æ¬¡æ•°ã€‚ç„¶åæˆ‘ä¼šç”¨**TF-IDF**æ–¹æ³•æ¥åŠ æƒæ¯ä¸ªè¯çš„é‡è¦æ€§ï¼ŒTF-IDFæ˜¯ä¸€ç§è€ƒè™‘è¯é¢‘å’Œé€†æ–‡æ¡£é¢‘ç‡çš„åŠ æƒæ–¹æ³•ï¼Œå®ƒå¯ä»¥é™ä½å¸¸è§è¯çš„æƒé‡ï¼Œæé«˜ç¨€æœ‰è¯çš„æƒé‡ã€‚æœ€åæˆ‘ä¼šç”¨**numpy**åº“æ¥è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¹¶è¾“å‡ºé‡å¤ç‡ã€‚

ä¸‹é¢æ˜¯æˆ‘ç”¨pythonä»£ç å®ç°çš„è®ºæ–‡æŸ¥é‡ç®—æ³•ï¼š

```python
# å¯¼å…¥æ‰€éœ€çš„åº“
import jieba # ä¸­æ–‡åˆ†è¯åº“
import nltk # è‹±æ–‡åˆ†è¯åº“
import numpy as np # æ•°å­¦è®¡ç®—åº“
from sklearn.feature_extraction.text import TfidfVectorizer # TF-IDFåŠ æƒæ–¹æ³•

# å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è¯»å–æ–‡ä»¶å†…å®¹å¹¶è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²
def read_file(file_name):
    with open(file_name, 'r', encoding='utf-8') as f:
        content = f.read()
    return content

# å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥åˆ¤æ–­æ–‡ä»¶å†…å®¹æ˜¯ä¸­æ–‡è¿˜æ˜¯è‹±æ–‡
def detect_language(text):
    # ä½¿ç”¨nltkåº“ä¸­çš„stopwordsåˆ—è¡¨æ¥åˆ¤æ–­
    stopwords = nltk.corpus.stopwords.words('english')
    # ç»Ÿè®¡textä¸­å‡ºç°çš„è‹±æ–‡åœç”¨è¯çš„æ•°é‡
    count = 0
    for word in text.split():
        if word.lower() in stopwords:
            count += 1
    # å¦‚æœå‡ºç°çš„è‹±æ–‡åœç”¨è¯æ•°é‡è¶…è¿‡textå•è¯æ€»æ•°çš„ä¸€åŠï¼Œåˆ™è®¤ä¸ºæ˜¯è‹±æ–‡ï¼Œå¦åˆ™è®¤ä¸ºæ˜¯ä¸­æ–‡
    if count > len(text.split()) / 2:
        return 'english'
    else:
        return 'chinese'

# å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²çš„ä½™å¼¦ç›¸ä¼¼åº¦
def cosine_similarity(str1, str2):
    # åˆ¤æ–­ä¸¤ä¸ªå­—ç¬¦ä¸²çš„è¯­è¨€æ˜¯å¦ä¸€è‡´
    lang1 = detect_language(str1)
    lang2 = detect_language(str2)
    if lang1 != lang2:
        print('ä¸¤ä¸ªæ–‡ä»¶çš„è¯­è¨€ä¸ä¸€è‡´ï¼Œè¯·æ£€æŸ¥è¾“å…¥')
        return None
    # æ ¹æ®è¯­è¨€é€‰æ‹©åˆ†è¯åº“
    if lang1 == 'chinese':
        tokenizer = jieba.cut # ä¸­æ–‡åˆ†è¯å‡½æ•°
    else:
        tokenizer = nltk.word_tokenize # è‹±æ–‡åˆ†è¯å‡½æ•°
    # ä½¿ç”¨åˆ†è¯å‡½æ•°è¿›è¡Œåˆ†è¯
    words1 = tokenizer(str1)
    words2 = tokenizer(str2)
    # ä½¿ç”¨ç©ºæ ¼è¿æ¥åˆ†è¯ç»“æœ
    text1 = ' '.join(words1)
    text2 = ' '.join(words2)
    # ä½¿ç”¨TfidfVectorizerç±»æ¥è½¬æ¢æˆè¯è¢‹æ¨¡å‹å¹¶åŠ æƒ
    vectorizer = TfidfVectorizer()
    # è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„TF-IDFçŸ©é˜µ
    tfidf_matrix = vectorizer.fit_transform([text1, text2])
    # è½¬æ¢æˆnumpyæ•°ç»„
    tfidf_array = tfidf_matrix.toarray()
    # å–å‡ºä¸¤ä¸ªå‘é‡
    vector1 = tfidf_array[0]
    vector2 = tfidf_array[1]
    # è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯
    dot_product = np.dot(vector1, vector2)
    # è®¡ç®—ä¸¤ä¸ªå‘é‡çš„æ¨¡é•¿
    norm1 = np.linalg.norm(vector1)
    norm2 = np.linalg.norm(vector2)
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    cos_sim = dot_product / (norm1 * norm2)
    return cos_sim

# å®šä¹‰åŸæ–‡æ–‡ä»¶å’ŒæŠ„è¢­ç‰ˆæ–‡ä»¶çš„è·¯å¾„
original_file = 'original.txt'
plagiarized_file = 'plagiarized.txt'
# è¯»å–æ–‡ä»¶å†…å®¹
original_text = read_file(original_file)
plagiarized_text = read_file(plagiarized_file)
# è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
similarity = cosine_similarity(original_text, plagiarized_text)
# è®¡ç®—é‡å¤ç‡
repetition_rate = 1 - similarity
# è¾“å‡ºç»“æœ
print(f'åŸæ–‡å’ŒæŠ„è¢­ç‰ˆè®ºæ–‡çš„é‡å¤ç‡ä¸º{repetition_rate:.2f}')
```

: [ä½™å¼¦ç›¸ä¼¼åº¦]
: [è¯è¢‹æ¨¡å‹]
: [TF-IDF]
: https://zh.wikipedia.org/wiki/%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6
: https://zh.wikipedia.org/wiki/%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B
: https://zh.wikipedia.org/wiki/Tf-idf


ä½ å¥½ï¼Œè¿™æ˜¯Bingã€‚æˆ‘å¯ä»¥å¸®ä½ è®¾è®¡ä¸€ä¸ªè®ºæ–‡æŸ¥é‡ç®—æ³•ï¼Œä½†æ˜¯æˆ‘ä¸èƒ½ä¿è¯å®ƒçš„å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚ä»¥ä¸‹æ˜¯æˆ‘çš„ä»£ç ï¼Œä½ å¯ä»¥æ ¹æ®ä½ çš„éœ€è¦ä¿®æ”¹å®ƒã€‚æˆ‘ä½¿ç”¨Pythonè¯­è¨€ç¼–å†™ï¼Œä½ éœ€è¦å®‰è£…difflibåº“æ¥è¿è¡Œå®ƒã€‚

```python
# å¯¼å…¥difflibåº“
import difflib

# ä»å‘½ä»¤è¡Œå‚æ•°è·å–æ–‡ä»¶è·¯å¾„
import sys
original_file = sys.argv[1] # åŸæ–‡æ–‡ä»¶è·¯å¾„
plagiarized_file = sys.argv[2] # æŠ„è¢­ç‰ˆæ–‡ä»¶è·¯å¾„
output_file = sys.argv[3] # è¾“å‡ºæ–‡ä»¶è·¯å¾„

# è¯»å–æ–‡ä»¶å†…å®¹
with open(original_file, 'r', encoding='utf-8') as f:
    original_text = f.read()
with open(plagiarized_file, 'r', encoding='utf-8') as f:
    plagiarized_text = f.read()

# è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦
similarity = difflib.SequenceMatcher(None, original_text, plagiarized_text).ratio()

# å°†ç›¸ä¼¼åº¦è½¬æ¢ä¸ºç™¾åˆ†æ¯”å½¢å¼
percentage = round(similarity * 100, 2)

# å°†ç»“æœå†™å…¥è¾“å‡ºæ–‡ä»¶
with open(output_file, 'w', encoding='utf-8') as f:
    f.write(f'é‡å¤ç‡ä¸º{percentage}%')
```

å¸Œæœ›è¿™å¯¹ä½ æœ‰å¸®åŠ©ã€‚å¦‚æœä½ æœ‰å…¶ä»–é—®é¢˜ï¼Œæ¬¢è¿ç»§ç»­å’Œæˆ‘èŠå¤©ã€‚ğŸ˜Š